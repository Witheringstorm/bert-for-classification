nohup: ignoring input
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Using device: cuda
Using local pre-trained model from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased

==================== TRAINING ON DOMAIN: ESSAY ====================
Loading training data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/essay/train_en.csv
Loading validation data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/essay/val_en.csv
Training data shape (essay): (1596, 2)
Validation data shape (essay): (200, 2)

Epoch 1/5 (Training on essay)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:06<02:45,  6.92s/it]  8%|▊         | 2/25 [00:08<01:20,  3.52s/it] 12%|█▏        | 3/25 [00:09<00:53,  2.43s/it] 16%|█▌        | 4/25 [00:10<00:40,  1.92s/it] 20%|██        | 5/25 [00:11<00:32,  1.64s/it] 24%|██▍       | 6/25 [00:12<00:27,  1.47s/it] 28%|██▊       | 7/25 [00:13<00:24,  1.36s/it] 32%|███▏      | 8/25 [00:14<00:22,  1.29s/it] 36%|███▌      | 9/25 [00:16<00:19,  1.25s/it] 40%|████      | 10/25 [00:17<00:18,  1.22s/it] 44%|████▍     | 11/25 [00:18<00:16,  1.19s/it] 48%|████▊     | 12/25 [00:19<00:15,  1.18s/it] 52%|█████▏    | 13/25 [00:20<00:14,  1.17s/it] 56%|█████▌    | 14/25 [00:21<00:12,  1.16s/it] 60%|██████    | 15/25 [00:22<00:11,  1.16s/it] 64%|██████▍   | 16/25 [00:24<00:10,  1.16s/it] 68%|██████▊   | 17/25 [00:25<00:09,  1.16s/it] 72%|███████▏  | 18/25 [00:26<00:08,  1.16s/it] 76%|███████▌  | 19/25 [00:27<00:06,  1.15s/it] 80%|████████  | 20/25 [00:28<00:05,  1.15s/it] 84%|████████▍ | 21/25 [00:29<00:04,  1.16s/it] 88%|████████▊ | 22/25 [00:31<00:03,  1.16s/it] 92%|█████████▏| 23/25 [00:32<00:02,  1.16s/it] 96%|█████████▌| 24/25 [00:33<00:01,  1.16s/it]100%|██████████| 25/25 [00:34<00:00,  1.14s/it]100%|██████████| 25/25 [00:34<00:00,  1.38s/it]
[LOG] Train Domain: essay | Epoch: 1 | Train Loss: 0.5560 | Train Accuracy: 0.7030
Evaluating on validation set for essay...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.73it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]
[LOG] Train Domain: essay | Epoch: 1 | Val Loss: 0.1019 | Val Accuracy: 0.9650
[LOG] Train Domain: essay | Epoch: 1 | Val F1 (Binary for LLM class): 0.9641
[LOG] New best validation F1 (0.9641) for essay at epoch 1. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_essay_en.bin

Epoch 2/5 (Training on essay)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:05<02:05,  5.23s/it]  8%|▊         | 2/25 [00:06<01:05,  2.84s/it] 12%|█▏        | 3/25 [00:07<00:45,  2.07s/it] 16%|█▌        | 4/25 [00:08<00:35,  1.71s/it] 20%|██        | 5/25 [00:09<00:30,  1.51s/it] 24%|██▍       | 6/25 [00:11<00:26,  1.40s/it] 28%|██▊       | 7/25 [00:12<00:23,  1.32s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.27s/it] 36%|███▌      | 9/25 [00:14<00:19,  1.24s/it] 40%|████      | 10/25 [00:15<00:18,  1.22s/it] 44%|████▍     | 11/25 [00:16<00:16,  1.20s/it] 48%|████▊     | 12/25 [00:18<00:15,  1.19s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.19s/it] 56%|█████▌    | 14/25 [00:20<00:12,  1.18s/it] 60%|██████    | 15/25 [00:21<00:11,  1.18s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.18s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.18s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.18s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.18s/it] 80%|████████  | 20/25 [00:27<00:05,  1.18s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.18s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.18s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.18s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.18s/it]100%|██████████| 25/25 [00:33<00:00,  1.15s/it]100%|██████████| 25/25 [00:33<00:00,  1.34s/it]
[LOG] Train Domain: essay | Epoch: 2 | Train Loss: 0.1503 | Train Accuracy: 0.9555
Evaluating on validation set for essay...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.31it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.70it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]
[LOG] Train Domain: essay | Epoch: 2 | Val Loss: 0.0353 | Val Accuracy: 0.9850
[LOG] Train Domain: essay | Epoch: 2 | Val F1 (Binary for LLM class): 0.9848
[LOG] New best validation F1 (0.9848) for essay at epoch 2. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_essay_en.bin

Epoch 3/5 (Training on essay)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:05<02:12,  5.52s/it]  8%|▊         | 2/25 [00:06<01:08,  2.97s/it] 12%|█▏        | 3/25 [00:07<00:47,  2.15s/it] 16%|█▌        | 4/25 [00:09<00:37,  1.76s/it] 20%|██        | 5/25 [00:10<00:31,  1.55s/it] 24%|██▍       | 6/25 [00:11<00:27,  1.43s/it] 28%|██▊       | 7/25 [00:12<00:24,  1.35s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.29s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:16<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:17<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:18<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.20s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:22<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:23<00:10,  1.19s/it] 68%|██████▊   | 17/25 [00:24<00:09,  1.19s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.19s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.19s/it] 80%|████████  | 20/25 [00:28<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:29<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:30<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.17s/it]100%|██████████| 25/25 [00:34<00:00,  1.37s/it]
[LOG] Train Domain: essay | Epoch: 3 | Train Loss: 0.0327 | Train Accuracy: 0.9906
Evaluating on validation set for essay...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.69it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
[LOG] Train Domain: essay | Epoch: 3 | Val Loss: 0.0021 | Val Accuracy: 1.0000
[LOG] Train Domain: essay | Epoch: 3 | Val F1 (Binary for LLM class): 1.0000
[LOG] New best validation F1 (1.0000) for essay at epoch 3. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_essay_en.bin

Epoch 4/5 (Training on essay)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:05<02:10,  5.45s/it]  8%|▊         | 2/25 [00:06<01:07,  2.94s/it] 12%|█▏        | 3/25 [00:07<00:47,  2.14s/it] 16%|█▌        | 4/25 [00:09<00:37,  1.76s/it] 20%|██        | 5/25 [00:10<00:31,  1.56s/it] 24%|██▍       | 6/25 [00:11<00:27,  1.43s/it] 28%|██▊       | 7/25 [00:12<00:24,  1.35s/it] 32%|███▏      | 8/25 [00:13<00:22,  1.30s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.27s/it] 40%|████      | 10/25 [00:16<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:17<00:17,  1.23s/it] 48%|████▊     | 12/25 [00:18<00:15,  1.22s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:22<00:12,  1.20s/it] 64%|██████▍   | 16/25 [00:23<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:24<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.19s/it] 80%|████████  | 20/25 [00:28<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:29<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:30<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.17s/it]100%|██████████| 25/25 [00:34<00:00,  1.37s/it]
[LOG] Train Domain: essay | Epoch: 4 | Train Loss: 0.0283 | Train Accuracy: 0.9931
Evaluating on validation set for essay...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.67it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Train Domain: essay | Epoch: 4 | Val Loss: 0.0072 | Val Accuracy: 0.9950
[LOG] Train Domain: essay | Epoch: 4 | Val F1 (Binary for LLM class): 0.9950
[LOG] Validation F1 did not improve for 1 epoch(s) for essay.
[LOG] Early stopping triggered for essay at epoch 4. Best F1 was 1.0000 at epoch 3.

Loading best model for essay (from epoch 3, Val F1: 1.0000) for cross-domain testing.

--- Testing on DOMAIN: ESSAY (Model trained on ESSAY) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/essay/test_en.csv
Test data shape (essay): (198, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.72it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.67it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on essay) on Test Set (essay):
[LOG] Test Loss: 0.2429 | Test Accuracy: 0.9848
[LOG] Test F1 (Binary for LLM class): 0.9851
[LOG] Test F1 (Macro): 0.9848
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.99      0.98      0.98        98\n[LOG]      llm (1)       0.98      0.99      0.99       100\n[LOG] \n[LOG]     accuracy                           0.98       198\n[LOG]    macro avg       0.98      0.98      0.98       198\n[LOG] weighted avg       0.98      0.98      0.98       198

--- Testing on DOMAIN: REUTER (Model trained on ESSAY) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/reuter/test_en.csv
Test data shape (reuter): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.39it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.77it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on essay) on Test Set (reuter):
[LOG] Test Loss: 2.7604 | Test Accuracy: 0.4900
[LOG] Test F1 (Binary for LLM class): 0.6383
[LOG] Test F1 (Macro): 0.3869
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.44      0.08      0.14       100\n[LOG]      llm (1)       0.49      0.90      0.64       100\n[LOG] \n[LOG]     accuracy                           0.49       200\n[LOG]    macro avg       0.47      0.49      0.39       200\n[LOG] weighted avg       0.47      0.49      0.39       200

--- Testing on DOMAIN: WP (Model trained on ESSAY) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/wp/test_en.csv
Test data shape (wp): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.39it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.77it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[LOG] Results for Model (Trained on essay) on Test Set (wp):
[LOG] Test Loss: 0.6589 | Test Accuracy: 0.8050
[LOG] Test F1 (Binary for LLM class): 0.8326
[LOG] Test F1 (Macro): 0.7995
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.96      0.64      0.77       100\n[LOG]      llm (1)       0.73      0.97      0.83       100\n[LOG] \n[LOG]     accuracy                           0.81       200\n[LOG]    macro avg       0.84      0.80      0.80       200\n[LOG] weighted avg       0.84      0.81      0.80       200

==================== TRAINING ON DOMAIN: REUTER ====================
Loading training data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/reuter/train_en.csv
Loading validation data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/reuter/val_en.csv
Training data shape (reuter): (1600, 2)
Validation data shape (reuter): (200, 2)

Epoch 1/5 (Training on reuter)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:04<01:52,  4.69s/it]  8%|▊         | 2/25 [00:05<01:00,  2.63s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.97s/it] 16%|█▌        | 4/25 [00:08<00:34,  1.66s/it] 20%|██        | 5/25 [00:09<00:29,  1.49s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.39s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.28s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:18<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:24<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.20s/it] 80%|████████  | 20/25 [00:27<00:05,  1.20s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.20s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.20s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.20s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: reuter | Epoch: 1 | Train Loss: 0.3585 | Train Accuracy: 0.8163
Evaluating on validation set for reuter...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.76it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]
[LOG] Train Domain: reuter | Epoch: 1 | Val Loss: 0.0639 | Val Accuracy: 0.9600
[LOG] Train Domain: reuter | Epoch: 1 | Val F1 (Binary for LLM class): 0.9615
[LOG] New best validation F1 (0.9615) for reuter at epoch 1. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_reuter_en.bin

Epoch 2/5 (Training on reuter)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:53,  4.72s/it]  8%|▊         | 2/25 [00:05<01:00,  2.64s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.98s/it] 16%|█▌        | 4/25 [00:08<00:35,  1.67s/it] 20%|██        | 5/25 [00:09<00:29,  1.50s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.39s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.29s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.22s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:12,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.20s/it] 80%|████████  | 20/25 [00:27<00:05,  1.20s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.20s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.20s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.20s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: reuter | Epoch: 2 | Train Loss: 0.1431 | Train Accuracy: 0.9544
Evaluating on validation set for reuter...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]
[LOG] Train Domain: reuter | Epoch: 2 | Val Loss: 0.0423 | Val Accuracy: 0.9800
[LOG] Train Domain: reuter | Epoch: 2 | Val F1 (Binary for LLM class): 0.9796
[LOG] New best validation F1 (0.9796) for reuter at epoch 2. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_reuter_en.bin

Epoch 3/5 (Training on reuter)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:53,  4.73s/it]  8%|▊         | 2/25 [00:05<01:00,  2.65s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.98s/it] 16%|█▌        | 4/25 [00:08<00:35,  1.67s/it] 20%|██        | 5/25 [00:09<00:30,  1.50s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.40s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.29s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:12,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.20s/it] 80%|████████  | 20/25 [00:27<00:05,  1.20s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.20s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.20s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.20s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: reuter | Epoch: 3 | Train Loss: 0.0140 | Train Accuracy: 0.9944
Evaluating on validation set for reuter...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]
[LOG] Train Domain: reuter | Epoch: 3 | Val Loss: 0.0408 | Val Accuracy: 0.9850
[LOG] Train Domain: reuter | Epoch: 3 | Val F1 (Binary for LLM class): 0.9852
[LOG] New best validation F1 (0.9852) for reuter at epoch 3. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_reuter_en.bin

Epoch 4/5 (Training on reuter)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:51,  4.63s/it]  8%|▊         | 2/25 [00:05<00:59,  2.61s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.96s/it] 16%|█▌        | 4/25 [00:08<00:34,  1.66s/it] 20%|██        | 5/25 [00:09<00:29,  1.49s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.39s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:12<00:21,  1.28s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:18<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:12,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:24<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.20s/it] 80%|████████  | 20/25 [00:27<00:05,  1.20s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.20s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.20s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.20s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: reuter | Epoch: 4 | Train Loss: 0.0046 | Train Accuracy: 0.9988
Evaluating on validation set for reuter...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.76it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]
[LOG] Train Domain: reuter | Epoch: 4 | Val Loss: 0.0025 | Val Accuracy: 1.0000
[LOG] Train Domain: reuter | Epoch: 4 | Val F1 (Binary for LLM class): 1.0000
[LOG] New best validation F1 (1.0000) for reuter at epoch 4. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_reuter_en.bin

Epoch 5/5 (Training on reuter)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:53,  4.74s/it]  8%|▊         | 2/25 [00:05<01:00,  2.65s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.99s/it] 16%|█▌        | 4/25 [00:08<00:35,  1.67s/it] 20%|██        | 5/25 [00:09<00:29,  1.50s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.40s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.29s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:12,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.20s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.20s/it] 80%|████████  | 20/25 [00:27<00:05,  1.20s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.20s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.20s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.20s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.20s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: reuter | Epoch: 5 | Train Loss: 0.0009 | Train Accuracy: 1.0000
Evaluating on validation set for reuter...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Train Domain: reuter | Epoch: 5 | Val Loss: 0.0026 | Val Accuracy: 1.0000
[LOG] Train Domain: reuter | Epoch: 5 | Val F1 (Binary for LLM class): 1.0000
[LOG] Validation F1 did not improve for 1 epoch(s) for reuter.
[LOG] Early stopping triggered for reuter at epoch 5. Best F1 was 1.0000 at epoch 4.

Loading best model for reuter (from epoch 4, Val F1: 1.0000) for cross-domain testing.

--- Testing on DOMAIN: ESSAY (Model trained on REUTER) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/essay/test_en.csv
Test data shape (essay): (198, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Evaluating:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]Evaluating: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on reuter) on Test Set (essay):
[LOG] Test Loss: 3.3068 | Test Accuracy: 0.5051
[LOG] Test F1 (Binary for LLM class): 0.6711
[LOG] Test F1 (Macro): 0.3356
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.00      0.00      0.00        98\n[LOG]      llm (1)       0.51      1.00      0.67       100\n[LOG] \n[LOG]     accuracy                           0.51       198\n[LOG]    macro avg       0.25      0.50      0.34       198\n[LOG] weighted avg       0.26      0.51      0.34       198

--- Testing on DOMAIN: REUTER (Model trained on REUTER) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/reuter/test_en.csv
Test data shape (reuter): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on reuter) on Test Set (reuter):
[LOG] Test Loss: 0.0110 | Test Accuracy: 0.9950
[LOG] Test F1 (Binary for LLM class): 0.9950
[LOG] Test F1 (Macro): 0.9950
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.99      1.00      1.00       100\n[LOG]      llm (1)       1.00      0.99      0.99       100\n[LOG] \n[LOG]     accuracy                           0.99       200\n[LOG]    macro avg       1.00      0.99      0.99       200\n[LOG] weighted avg       1.00      0.99      0.99       200

--- Testing on DOMAIN: WP (Model trained on REUTER) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/wp/test_en.csv
Test data shape (wp): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.74it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[LOG] Results for Model (Trained on reuter) on Test Set (wp):
[LOG] Test Loss: 0.2889 | Test Accuracy: 0.9000
[LOG] Test F1 (Binary for LLM class): 0.9065
[LOG] Test F1 (Macro): 0.8995
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.97      0.83      0.89       100\n[LOG]      llm (1)       0.85      0.97      0.91       100\n[LOG] \n[LOG]     accuracy                           0.90       200\n[LOG]    macro avg       0.91      0.90      0.90       200\n[LOG] weighted avg       0.91      0.90      0.90       200

==================== TRAINING ON DOMAIN: WP ====================
Loading training data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/wp/train_en.csv
Loading validation data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/wp/val_en.csv
Training data shape (wp): (1600, 2)
Validation data shape (wp): (200, 2)

Epoch 1/5 (Training on wp)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:04<01:47,  4.47s/it]  8%|▊         | 2/25 [00:05<00:58,  2.54s/it] 12%|█▏        | 3/25 [00:06<00:42,  1.92s/it] 16%|█▌        | 4/25 [00:08<00:34,  1.63s/it] 20%|██        | 5/25 [00:09<00:29,  1.47s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.38s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.32s/it] 32%|███▏      | 8/25 [00:12<00:21,  1.28s/it] 36%|███▌      | 9/25 [00:13<00:19,  1.25s/it] 40%|████      | 10/25 [00:15<00:18,  1.23s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:18<00:14,  1.20s/it] 56%|█████▌    | 14/25 [00:19<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.19s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.19s/it] 72%|███████▏  | 18/25 [00:24<00:08,  1.19s/it] 76%|███████▌  | 19/25 [00:25<00:07,  1.19s/it] 80%|████████  | 20/25 [00:27<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:31<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.33s/it]
[LOG] Train Domain: wp | Epoch: 1 | Train Loss: 0.4739 | Train Accuracy: 0.7769
Evaluating on validation set for wp...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]
[LOG] Train Domain: wp | Epoch: 1 | Val Loss: 0.0702 | Val Accuracy: 0.9750
[LOG] Train Domain: wp | Epoch: 1 | Val F1 (Binary for LLM class): 0.9746
[LOG] New best validation F1 (0.9746) for wp at epoch 1. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_wp_en.bin

Epoch 2/5 (Training on wp)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:53,  4.73s/it]  8%|▊         | 2/25 [00:05<01:00,  2.65s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.98s/it] 16%|█▌        | 4/25 [00:08<00:34,  1.67s/it] 20%|██        | 5/25 [00:09<00:29,  1.50s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.39s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.28s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.25s/it] 40%|████      | 10/25 [00:15<00:18,  1.23s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.19s/it] 72%|███████▏  | 18/25 [00:24<00:08,  1.19s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.19s/it] 80%|████████  | 20/25 [00:27<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: wp | Epoch: 2 | Train Loss: 0.1259 | Train Accuracy: 0.9513
Evaluating on validation set for wp...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.74it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]
[LOG] Train Domain: wp | Epoch: 2 | Val Loss: 0.0251 | Val Accuracy: 0.9900
[LOG] Train Domain: wp | Epoch: 2 | Val F1 (Binary for LLM class): 0.9901
[LOG] New best validation F1 (0.9901) for wp at epoch 2. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_wp_en.bin

Epoch 3/5 (Training on wp)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:04<01:54,  4.79s/it]  8%|▊         | 2/25 [00:05<01:01,  2.67s/it] 12%|█▏        | 3/25 [00:07<00:43,  1.99s/it] 16%|█▌        | 4/25 [00:08<00:35,  1.67s/it] 20%|██        | 5/25 [00:09<00:30,  1.50s/it] 24%|██▍       | 6/25 [00:10<00:26,  1.40s/it] 28%|██▊       | 7/25 [00:11<00:23,  1.33s/it] 32%|███▏      | 8/25 [00:13<00:21,  1.29s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:15<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:16<00:17,  1.22s/it] 48%|████▊     | 12/25 [00:17<00:15,  1.21s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:21<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:22<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:23<00:09,  1.20s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.19s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.19s/it] 80%|████████  | 20/25 [00:27<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:28<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:29<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.35s/it]
[LOG] Train Domain: wp | Epoch: 3 | Train Loss: 0.0727 | Train Accuracy: 0.9781
Evaluating on validation set for wp...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:05,  1.89s/it]Evaluating:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]Evaluating: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
[LOG] Train Domain: wp | Epoch: 3 | Val Loss: 0.0191 | Val Accuracy: 0.9950
[LOG] Train Domain: wp | Epoch: 3 | Val F1 (Binary for LLM class): 0.9950
[LOG] New best validation F1 (0.9950) for wp at epoch 3. Model saved to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/best_model_state_wp_en.bin

Epoch 4/5 (Training on wp)
--------------------
  0%|          | 0/25 [00:00<?, ?it/s]/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  4%|▍         | 1/25 [00:05<02:10,  5.44s/it]  8%|▊         | 2/25 [00:06<01:07,  2.94s/it] 12%|█▏        | 3/25 [00:07<00:47,  2.14s/it] 16%|█▌        | 4/25 [00:09<00:37,  1.76s/it] 20%|██        | 5/25 [00:10<00:31,  1.56s/it] 24%|██▍       | 6/25 [00:11<00:27,  1.43s/it] 28%|██▊       | 7/25 [00:12<00:24,  1.35s/it] 32%|███▏      | 8/25 [00:13<00:22,  1.30s/it] 36%|███▌      | 9/25 [00:14<00:20,  1.26s/it] 40%|████      | 10/25 [00:16<00:18,  1.24s/it] 44%|████▍     | 11/25 [00:17<00:17,  1.23s/it] 48%|████▊     | 12/25 [00:18<00:15,  1.22s/it] 52%|█████▏    | 13/25 [00:19<00:14,  1.21s/it] 56%|█████▌    | 14/25 [00:20<00:13,  1.20s/it] 60%|██████    | 15/25 [00:22<00:11,  1.20s/it] 64%|██████▍   | 16/25 [00:23<00:10,  1.20s/it] 68%|██████▊   | 17/25 [00:24<00:09,  1.19s/it] 72%|███████▏  | 18/25 [00:25<00:08,  1.19s/it] 76%|███████▌  | 19/25 [00:26<00:07,  1.19s/it] 80%|████████  | 20/25 [00:28<00:05,  1.19s/it] 84%|████████▍ | 21/25 [00:29<00:04,  1.19s/it] 88%|████████▊ | 22/25 [00:30<00:03,  1.19s/it] 92%|█████████▏| 23/25 [00:31<00:02,  1.19s/it] 96%|█████████▌| 24/25 [00:32<00:01,  1.19s/it]100%|██████████| 25/25 [00:33<00:00,  1.19s/it]100%|██████████| 25/25 [00:34<00:00,  1.37s/it]
[LOG] Train Domain: wp | Epoch: 4 | Train Loss: 0.0091 | Train Accuracy: 0.9969
Evaluating on validation set for wp...
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Train Domain: wp | Epoch: 4 | Val Loss: 0.0689 | Val Accuracy: 0.9800
[LOG] Train Domain: wp | Epoch: 4 | Val F1 (Binary for LLM class): 0.9804
[LOG] Validation F1 did not improve for 1 epoch(s) for wp.
[LOG] Early stopping triggered for wp at epoch 4. Best F1 was 0.9950 at epoch 3.

Loading best model for wp (from epoch 3, Val F1: 0.9950) for cross-domain testing.

--- Testing on DOMAIN: ESSAY (Model trained on WP) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/essay/test_en.csv
Test data shape (essay): (198, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s]Evaluating:  75%|███████▌  | 3/4 [00:02<00:00,  1.68it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on wp) on Test Set (essay):
[LOG] Test Loss: 1.4171 | Test Accuracy: 0.6212
[LOG] Test F1 (Binary for LLM class): 0.7273
[LOG] Test F1 (Macro): 0.5537
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       1.00      0.23      0.38        98\n[LOG]      llm (1)       0.57      1.00      0.73       100\n[LOG] \n[LOG]     accuracy                           0.62       198\n[LOG]    macro avg       0.79      0.62      0.55       198\n[LOG] weighted avg       0.78      0.62      0.56       198

--- Testing on DOMAIN: REUTER (Model trained on WP) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/reuter/test_en.csv
Test data shape (reuter): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]
/public/home/qinxiaoyu/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[LOG] Results for Model (Trained on wp) on Test Set (reuter):
[LOG] Test Loss: 0.5862 | Test Accuracy: 0.7700
[LOG] Test F1 (Binary for LLM class): 0.8099
[LOG] Test F1 (Macro): 0.7594
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.97      0.56      0.71       100\n[LOG]      llm (1)       0.69      0.98      0.81       100\n[LOG] \n[LOG]     accuracy                           0.77       200\n[LOG]    macro avg       0.83      0.77      0.76       200\n[LOG] weighted avg       0.83      0.77      0.76       200

--- Testing on DOMAIN: WP (Model trained on WP) ---
Loading test data from: /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/data/processed/wp/test_en.csv
Test data shape (wp): (200, 2)
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.36it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]
[LOG] Results for Model (Trained on wp) on Test Set (wp):
[LOG] Test Loss: 0.0894 | Test Accuracy: 0.9650
[LOG] Test F1 (Binary for LLM class): 0.9652
[LOG] Test F1 (Macro): 0.9650
[LOG] Classification Report (Test Set):
[LOG]               precision    recall  f1-score   support\n[LOG] \n[LOG]    human (0)       0.97      0.96      0.96       100\n[LOG]      llm (1)       0.96      0.97      0.97       100\n[LOG] \n[LOG]     accuracy                           0.96       200\n[LOG]    macro avg       0.97      0.96      0.96       200\n[LOG] weighted avg       0.97      0.96      0.96       200

{'='*20} CROSS-DOMAIN EVALUATION COMPLETE {'='*20}

Summary of All Cross-Domain Evaluations:

Trained on: essay, Tested on: essay (Best model from epoch 3 of essay training)
  Accuracy: 0.9848, F1 (Binary): 0.9851, F1 (Macro): 0.9848
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.99      0.98      0.98        98
     llm (1)       0.98      0.99      0.99       100

    accuracy                           0.98       198
   macro avg       0.98      0.98      0.98       198
weighted avg       0.98      0.98      0.98       198


Trained on: essay, Tested on: reuter (Best model from epoch 3 of essay training)
  Accuracy: 0.4900, F1 (Binary): 0.6383, F1 (Macro): 0.3869
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.44      0.08      0.14       100
     llm (1)       0.49      0.90      0.64       100

    accuracy                           0.49       200
   macro avg       0.47      0.49      0.39       200
weighted avg       0.47      0.49      0.39       200


Trained on: essay, Tested on: wp (Best model from epoch 3 of essay training)
  Accuracy: 0.8050, F1 (Binary): 0.8326, F1 (Macro): 0.7995
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.96      0.64      0.77       100
     llm (1)       0.73      0.97      0.83       100

    accuracy                           0.81       200
   macro avg       0.84      0.80      0.80       200
weighted avg       0.84      0.81      0.80       200


Trained on: reuter, Tested on: essay (Best model from epoch 4 of reuter training)
  Accuracy: 0.5051, F1 (Binary): 0.6711, F1 (Macro): 0.3356
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.00      0.00      0.00        98
     llm (1)       0.51      1.00      0.67       100

    accuracy                           0.51       198
   macro avg       0.25      0.50      0.34       198
weighted avg       0.26      0.51      0.34       198


Trained on: reuter, Tested on: reuter (Best model from epoch 4 of reuter training)
  Accuracy: 0.9950, F1 (Binary): 0.9950, F1 (Macro): 0.9950
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.99      1.00      1.00       100
     llm (1)       1.00      0.99      0.99       100

    accuracy                           0.99       200
   macro avg       1.00      0.99      0.99       200
weighted avg       1.00      0.99      0.99       200


Trained on: reuter, Tested on: wp (Best model from epoch 4 of reuter training)
  Accuracy: 0.9000, F1 (Binary): 0.9065, F1 (Macro): 0.8995
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.97      0.83      0.89       100
     llm (1)       0.85      0.97      0.91       100

    accuracy                           0.90       200
   macro avg       0.91      0.90      0.90       200
weighted avg       0.91      0.90      0.90       200


Trained on: wp, Tested on: essay (Best model from epoch 3 of wp training)
  Accuracy: 0.6212, F1 (Binary): 0.7273, F1 (Macro): 0.5537
  Classification Report:
              precision    recall  f1-score   support

   human (0)       1.00      0.23      0.38        98
     llm (1)       0.57      1.00      0.73       100

    accuracy                           0.62       198
   macro avg       0.79      0.62      0.55       198
weighted avg       0.78      0.62      0.56       198


Trained on: wp, Tested on: reuter (Best model from epoch 3 of wp training)
  Accuracy: 0.7700, F1 (Binary): 0.8099, F1 (Macro): 0.7594
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.97      0.56      0.71       100
     llm (1)       0.69      0.98      0.81       100

    accuracy                           0.77       200
   macro avg       0.83      0.77      0.76       200
weighted avg       0.83      0.77      0.76       200


Trained on: wp, Tested on: wp (Best model from epoch 3 of wp training)
  Accuracy: 0.9650, F1 (Binary): 0.9652, F1 (Macro): 0.9650
  Classification Report:
              precision    recall  f1-score   support

   human (0)       0.97      0.96      0.96       100
     llm (1)       0.96      0.97      0.97       100

    accuracy                           0.96       200
   macro avg       0.97      0.96      0.96       200
weighted avg       0.97      0.96      0.96       200


[LOG] Saved all cross-domain evaluation results to /public/share/yinxiangrong/qinxiaoyu/kdc2024/S/nlp/proj/bert_en/models/cross_domain_results_en.json
